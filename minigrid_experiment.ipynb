{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cb1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from typing import Dict\n",
    "import multigrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d57836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obss_preprocessor(obs_space):\n",
    "    # Check if obs_space is an image space\n",
    "    if isinstance(obs_space, gym.spaces.Box):\n",
    "        obs_space = {\"image\": obs_space.shape}\n",
    "\n",
    "        def preprocess_obss(obss, device=None):\n",
    "            return torch_ac.DictList({\n",
    "                \"image\": preprocess_images(obss, device=device)\n",
    "            })\n",
    "\n",
    "    # Check if it is a MiniGrid observation space\n",
    "    elif isinstance(obs_space, gym.spaces.Dict) and \"image\" in obs_space.spaces.keys():\n",
    "        obs_space = {\"image\": obs_space.spaces[\"image\"].shape, \"text\": 100}\n",
    "\n",
    "        vocab = Vocabulary(obs_space[\"text\"])\n",
    "\n",
    "        def preprocess_obss(obss, device=None):\n",
    "            return torch_ac.DictList({\n",
    "                \"image\": preprocess_images([obs[\"image\"] for obs in obss], device=device),\n",
    "                \"text\": preprocess_texts([obs[\"mission\"] for obs in obss], vocab, device=device)\n",
    "            })\n",
    "\n",
    "        preprocess_obss.vocab = vocab\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown observation space: \" + str(obs_space))\n",
    "\n",
    "    return obs_space, preprocess_obss\n",
    "\n",
    "\n",
    "def preprocess_images(images, device=None):\n",
    "    # Bug of Pytorch: very slow if not first converted to numpy array\n",
    "    images = numpy.array(images)\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f572c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 64, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        print(observation_space)\n",
    "        \n",
    "        n = observation_space.shape[0]\n",
    "        m = observation_space.shape[1]\n",
    "        print(n,m)\n",
    "        self.image_embedding_size = ((n-1)//2-2)*((m-1)//2-2)*64\n",
    "\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        tens = torch.as_tensor(observation_space.sample()[None]).to(torch.uint8).float().permute(0,3,1,2)\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(tens).shape[1]\n",
    "        print(features_dim, self.image_embedding_size)\n",
    "        lin = nn.Linear(n_flatten, features_dim)\n",
    "        self.linear = nn.Sequential(lin, nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        print(f\"from_forward{observations.shape}\")\n",
    "        observations = torch.Tensor(observations)\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a978714",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb1a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniGrid-Empty-6x6-v0\", render_mode=\"rgb_array\")\n",
    "env = ImgObsWrapper(env)\n",
    "obs = env.reset()\n",
    "obs = obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f4df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multigrid.envs import EmptyEnv\n",
    "from multigrid.wrappers import MultiAgentImgObsWrapper\n",
    "\n",
    "env2 = EmptyEnv(render_mode=\"rgb_array\", agents=2)\n",
    "env2 = MultiAgentImgObsWrapper(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47b612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tens = torch.as_tensor(env.observation_space.sample()[None]).to(torch.uint8).float().permute(0,3,1,2)\n",
    "\n",
    "\n",
    "# tens2 = torch.as_tensor(env2.observation_space[0].sample()[None]).to(torch.uint8).float().permute(0,3,1,2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913d676c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[107., 170., 117., 146.,  40., 116., 166.],\n",
       "          [ 54., 163., 225.,  73.,  52.,   9.,  64.],\n",
       "          [188.,   3.,  21., 248.,  68., 166., 120.],\n",
       "          [178., 152.,  85.,  33.,  47., 134., 125.],\n",
       "          [ 39., 151.,  33.,   3.,  94., 187., 249.],\n",
       "          [143., 177.,  21., 173.,  95.,  33., 205.],\n",
       "          [ 33.,  27., 234.,  18., 148.,  40.,   4.]],\n",
       "\n",
       "         [[ 30., 160.,  59.,  11.,  21., 212.,  65.],\n",
       "          [253., 129.,  61., 187.,  14., 210.,  84.],\n",
       "          [169.,  52.,   0.,  49., 252.,   9.,   4.],\n",
       "          [228.,  84., 103., 251., 203., 253., 165.],\n",
       "          [159.,  50.,  43., 225., 250.,  50., 127.],\n",
       "          [ 99., 110.,  46.,  58., 219., 175., 152.],\n",
       "          [ 78., 199.,  15., 129., 103., 214.,  90.]],\n",
       "\n",
       "         [[ 11., 114.,  35.,  80.,  25., 171.,  37.],\n",
       "          [ 12.,  72., 177., 207.,  94., 145., 175.],\n",
       "          [199., 197., 186.,  42., 191., 213., 105.],\n",
       "          [166.,  77., 208.,  38.,  97.,  43., 216.],\n",
       "          [ 78., 129., 108., 161., 178.,  42., 251.],\n",
       "          [205., 215.,  62., 137., 214.,  93.,  27.],\n",
       "          [ 52., 174.,  45., 112., 241.,  65., 246.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84470b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[229., 207., 107.,  20., 112., 114.,  58.],\n",
       "          [236., 221., 208., 161., 209., 139.,  22.],\n",
       "          [ 91., 109., 214., 255., 226., 169., 160.],\n",
       "          [121., 147., 150., 214.,   3., 254., 162.],\n",
       "          [ 27.,  76.,  81., 112., 164., 227.,  23.],\n",
       "          [116.,  10.,  30., 241., 201., 195., 153.],\n",
       "          [ 82.,  19.,  35., 228.,  80., 207., 135.]],\n",
       "\n",
       "         [[ 29., 130.,  92., 241., 146., 221.,  81.],\n",
       "          [ 93., 201., 159., 229., 214.,  77., 116.],\n",
       "          [ 61.,  42.,  12.,  34., 249.,  87., 147.],\n",
       "          [207., 238., 200., 180., 169., 212.,  45.],\n",
       "          [ 64., 238., 137., 114., 203.,  61., 203.],\n",
       "          [ 68.,  84.,  16.,  36., 162.,  70.,  54.],\n",
       "          [ 79., 163., 161.,   5.,  29., 189., 180.]],\n",
       "\n",
       "         [[187., 218., 115.,  66., 153.,   6.,  83.],\n",
       "          [ 93.,  42., 212., 147., 140., 168.,  94.],\n",
       "          [152., 147.,  91., 105., 209., 127., 116.],\n",
       "          [103.,  35., 157.,  50., 198., 104., 240.],\n",
       "          [ 98.,  26., 244.,  79., 219., 246.,  47.],\n",
       "          [124., 233., 226., 147., 136., 178., 160.],\n",
       "          [197., 187., 204.,  37.,   7.,  70., 104.]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71913a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 5.5412, 0.0000, 0.0000]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cnn(tens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdbcea90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 72.,  68.,  48., 215., 167.,  59., 180.],\n",
       "          [168., 183., 216., 245., 227.,  24., 229.],\n",
       "          [213.,  72., 175., 236.,  96., 140., 174.],\n",
       "          [ 40., 212., 101.,  10.,  50., 154., 103.],\n",
       "          [189.,  97.,  31., 155.,  40., 194.,  88.],\n",
       "          [100., 203., 187., 247., 199.,  16.,  19.],\n",
       "          [153.,  27.,  93.,   1.,   7., 184., 135.]],\n",
       "\n",
       "         [[253., 249.,  36., 190., 205.,  88., 162.],\n",
       "          [ 12., 215., 169.,  15., 221., 236.,  39.],\n",
       "          [202., 124., 230., 211.,  41., 156., 166.],\n",
       "          [166., 217., 144., 212., 224.,  99., 200.],\n",
       "          [107., 108., 208.,  13., 153.,  34., 220.],\n",
       "          [ 73., 230., 180., 224., 111., 186., 196.],\n",
       "          [138.,   5.,  70.,  82., 213., 135., 214.]],\n",
       "\n",
       "         [[157.,  61., 181.,  23., 107., 173., 143.],\n",
       "          [233., 186., 243., 136.,  53., 148., 146.],\n",
       "          [213., 119., 144., 184., 124., 202.,  29.],\n",
       "          [161., 169., 179., 112., 115., 162., 198.],\n",
       "          [ 25., 105., 166., 109.,  59., 236., 200.],\n",
       "          [254.,   3.,   0., 248.,  77., 250.,  47.],\n",
       "          [ 29., 254., 219.,  31., 248., 167.,  52.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735cea8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from multigrid.envs import EmptyEnv\n",
    "from multigrid.wrappers import MultiAgentImgObsWrapper\n",
    "\n",
    "env2 = EmptyEnv(render_mode=\"rgb_array\", agents=2)\n",
    "env2 = MultiAgentImgObsWrapper(env2)\n",
    "\n",
    "obs2 = env2.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4b0f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m feature_extractor  \u001b[39m=\u001b[39m MinigridFeaturesExtractor(env2\u001b[39m.\u001b[39mobservation_space[\u001b[39m0\u001b[39m], features_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env2' is not defined"
     ]
    }
   ],
   "source": [
    "feature_extractor  = MinigridFeaturesExtractor(env2.observation_space[0], features_dim=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 64, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        print(observation_space)\n",
    "        \n",
    "        n = observation_space.shape[0]\n",
    "        m = observation_space.shape[1]\n",
    "        print(n,m)\n",
    "        self.image_embedding_size = ((n-1)//2-2)*((m-1)//2-2)*64\n",
    "\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        tens = torch.as_tensor(observation_space.sample()[None]).to(torch.uint8).float().permute(0,3,1,2)\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(tens).shape[1]\n",
    "        print(features_dim, self.image_embedding_size)\n",
    "        lin = nn.Linear(n_flatten, features_dim)\n",
    "        self.linear = nn.Sequential(lin, nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        print(f\"from_forward{observations.shape}\")\n",
    "        observations = torch.Tensor(observations)\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 64, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        print(observation_space)\n",
    "        \n",
    "        n = observation_space.shape[0]\n",
    "        m = observation_space.shape[1]\n",
    "        print(n,m)\n",
    "        self.image_embedding_size = ((n-1)//2-2)*((m-1)//2-2)*64\n",
    "\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        tens = torch.as_tensor(observation_space.sample()[None]).to(torch.uint8).float().permute(0,3,1,2)\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(tens).shape[1]\n",
    "        print(features_dim, self.image_embedding_size)\n",
    "        lin = nn.Linear(n_flatten, features_dim)\n",
    "        self.linear = nn.Sequential(lin, nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        print(f\"from_forward{observations.shape}\")\n",
    "        observations = torch.Tensor(observations)\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba190831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (7, 7, 3), int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.observation_space[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3242cf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (7, 7, 3), uint8)\n",
      "7 7\n",
      "128 64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m obs \u001b[39m=\u001b[39m obs[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m feature_extractor  \u001b[39m=\u001b[39m MinigridFeaturesExtractor(env\u001b[39m.\u001b[39mobservation_space, features_dim\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     action, _states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bensturgeon/werk/multigrid/minigrid_experiment.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     obs, rewards, done, info, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MiniGrid-Empty-6x6-v0\", render_mode=\"rgb_array\")\n",
    "env = ImgObsWrapper(env)\n",
    "obs = env.reset()\n",
    "obs = obs[0]\n",
    "\n",
    "feature_extractor  = MinigridFeaturesExtractor(env.observation_space, features_dim=128)\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info, _ = env.step(action)\n",
    "    obs_copy = obs\n",
    "    tensor_obs = torch.tensor(obs).float().permute(2,0,1).unsqueeze(0)\n",
    "    features = feature_extractor(tensor_obs)\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "512f4748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 7, 7])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "048b90c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multigrid_stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
