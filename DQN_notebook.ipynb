{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a20a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 309\u001b[0m\n\u001b[1;32m    307\u001b[0m env \u001b[39m=\u001b[39m RedBlueDoorsEnv(size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m    308\u001b[0m agent \u001b[39m=\u001b[39m Agent(env)\n\u001b[0;32m--> 309\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(\u001b[39m20\u001b[39;49m,env)\n\u001b[1;32m    310\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwe did it\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 231\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, num_episodes, env)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mfor\u001b[39;00m i_episode \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m    229\u001b[0m     \u001b[39m# Initialize the environment and get it's state\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     state, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 231\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(state, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32, device\u001b[39m=\u001b[39;49mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    232\u001b[0m     accumulated_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    233\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not dict"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "\n",
    "from multigrid.envs.empty import EmptyEnv\n",
    "\n",
    "from features_extractor import MinigridFeaturesExtractor\n",
    "import torch as T\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque , namedtuple\n",
    "from itertools import count\n",
    "# set up matplotlib\n",
    "from IPython import display\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(network, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, fc1_dims)\n",
    "        self.layer2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.layer3 = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class replay_memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, env) -> None:\n",
    "        n_actions = env.action_space[0].n\n",
    "        state, info = env.reset()\n",
    "        n_observations = len(state)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.policy_network =  network(n_observations = n_observations, n_actions=n_actions).to(self.device)\n",
    "        self.target_network = network(n_observations = n_observations, n_actions=n_actions).to(self.device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.memory = replay_memory(100000)\n",
    "        self.steps_done = 0\n",
    "        self.batch_size = 128\n",
    "        self.episode_durations = []\n",
    "        self.loss_record = []\n",
    "        self.episode_rewards = []\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 0.9\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = 1000\n",
    "        self.tau = 0.005\n",
    "        self.LR = 0.001\n",
    "\n",
    "        self.max_score = -math.inf\n",
    "        self.min_score = math.inf\n",
    "        self.avg_score = 0\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.LR, amsgrad=True)\n",
    "        \n",
    "        self.chkpt_file = 'tmp/lunar_lander'\n",
    "\n",
    "\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.network.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "        \n",
    "    \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with T.no_grad():\n",
    "                return self.policy_network(state).to(self.device).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return T.tensor([[env.action_space.sample()]], device=self.device, dtype=T.long)\n",
    "\n",
    "    def plot_durations(self, show_result=False):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        durations_t = T.tensor(self.episode_durations, dtype=T.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = T.cat((T.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "\n",
    "        plt.pause(0.001)\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                # display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    def plot_rewards(self, show_result=False):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        rewards_t = T.tensor(self.episode_rewards, dtype=T.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.plot(rewards_t.numpy())\n",
    "\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(rewards_t) >= 100:\n",
    "            means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = T.cat((T.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "\n",
    "        plt.pause(0.001)\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                # display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    def plot_loss(self, show_result=False):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        loss_t = T.tensor(self.loss_record, dtype=T.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(loss_t.numpy())\n",
    "        plt.pause(0.001)\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            # display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = T.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=T.bool)\n",
    "        non_final_next_states = T.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = T.cat(batch.state)\n",
    "        action_batch = T.cat(batch.action)\n",
    "        reward_batch = T.cat(batch.reward)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_network\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_network; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = T.zeros(self.batch_size, device=device)\n",
    "        with T.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.loss_record.append(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        T.nn.utils.clip_grad_value_(self.policy_network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    \n",
    "    def train(self, num_episodes, env):\n",
    "        \n",
    "        pbar = tqdm(range(1,num_episodes))\n",
    "        for i_episode in pbar:\n",
    "            # Initialize the environment and get it's state\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            accumulated_reward = 0\n",
    "            for t in count():\n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                accumulated_reward += reward\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                self.optimize_model()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = self.target_network.state_dict()\n",
    "                policy_net_state_dict = self.policy_network.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "                self.target_network.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_rewards.append(accumulated_reward)\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    break\n",
    "\n",
    "        print('Complete')\n",
    "        self.plot_durations()\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        self.plot_loss()\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        self.plot_rewards()\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "    \n",
    "    # Run episode\n",
    "    def run_episode(self, env, num_episodes=1):\n",
    "        total_reward = []\n",
    "        for episode in range(num_episodes):\n",
    "            episode_reward = 0\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            terminated = False\n",
    "            while not terminated: \n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                episode_reward += reward\n",
    "            total_reward.append(episode_reward)\n",
    "\n",
    "        return np.mean(total_reward)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # env = gym.make(\n",
    "    #     \"LunarLander-v2\",\n",
    "    #     continuous = False,\n",
    "    #     gravity = -10.0,\n",
    "    #     enable_wind = False,\n",
    "    #     wind_power = 15.0,\n",
    "    #     turbulence_power = 1.5\n",
    "    # )\n",
    "\n",
    "    env = RedBlueDoorsEnv(size=8)\n",
    "    agent = Agent(env)\n",
    "    agent.train(20,env)\n",
    "    print(\"we did it\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e1286b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "\n",
    "from multigrid.envs.redbluedoors import RedBlueDoorsEnv\n",
    "\n",
    "import torch as T\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque , namedtuple\n",
    "from itertools import count\n",
    "# set up matplotlib\n",
    "from IPython import display\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(network, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, fc1_dims)\n",
    "        self.layer2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.layer3 = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class replay_memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, env) -> None:\n",
    "        n_actions = env.action_space\n",
    "        state, info = env.reset()\n",
    "        n_observations = len(state)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.policy_network =  network(n_observations = n_observations, n_actions=n_actions).to(self.device)\n",
    "        self.target_network = network(n_observations = n_observations, n_actions=n_actions).to(self.device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.memory = replay_memory(100000)\n",
    "        self.steps_done = 0\n",
    "        self.batch_size = 128\n",
    "        self.episode_durations = []\n",
    "        self.loss_record = []\n",
    "        self.episode_rewards = []\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 0.9\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = 1000\n",
    "        self.tau = 0.005\n",
    "        self.LR = 0.001\n",
    "\n",
    "        self.max_score = -math.inf\n",
    "        self.min_score = math.inf\n",
    "        self.avg_score = 0\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.LR, amsgrad=True)\n",
    "        \n",
    "        self.chkpt_file = 'tmp/lunar_lander'\n",
    "\n",
    "\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.network.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "        \n",
    "    \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with T.no_grad():\n",
    "                return self.policy_network(state).to(self.device).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return T.tensor([[env.action_space.sample()]], device=self.device, dtype=T.long)\n",
    "\n",
    "    def plot_durations(self, show_result=False):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        durations_t = T.tensor(self.episode_durations, dtype=T.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = T.cat((T.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "\n",
    "        plt.pause(0.001)\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                # display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    def plot_rewards(self, show_result=False):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        rewards_t = T.tensor(self.episode_rewards, dtype=T.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.plot(rewards_t.numpy())\n",
    "\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(rewards_t) >= 100:\n",
    "            means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = T.cat((T.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "\n",
    "        plt.pause(0.001)\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                # display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    def plot_loss(self, show_result=False):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        loss_t = T.tensor(self.loss_record, dtype=T.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(loss_t.numpy())\n",
    "        plt.pause(0.001)\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            # display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = T.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=T.bool)\n",
    "        non_final_next_states = T.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = T.cat(batch.state)\n",
    "        action_batch = T.cat(batch.action)\n",
    "        reward_batch = T.cat(batch.reward)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        #for each batch state according to policy_network\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_network; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = T.zeros(self.batch_size, device=device) \n",
    "        with T.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.loss_record.append(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        T.nn.utils.clip_grad_value_(self.policy_network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    \n",
    "    def train(self, num_episodes, env):\n",
    "        \n",
    "        pbar = tqdm(range(1,num_episodes))\n",
    "        for i_episode in pbar:\n",
    "            # Initialize the environment and get it's state\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            accumulated_reward = 0\n",
    "            for t in count():\n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                accumulated_reward += reward\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                self.optimize_model()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = self.target_network.state_dict()\n",
    "                policy_net_state_dict = self.policy_network.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "                self.target_network.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_rewards.append(accumulated_reward)\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    break\n",
    "\n",
    "        print('Complete')\n",
    "        self.plot_durations()\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        self.plot_loss()\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        self.plot_rewards()\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "    \n",
    "    # Run episode\n",
    "    def run_episode(self, env, num_episodes=1):\n",
    "        total_reward = []\n",
    "        for episode in range(num_episodes):\n",
    "            episode_reward = 0\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            terminated = False\n",
    "            while not terminated: \n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                episode_reward += reward\n",
    "            total_reward.append(episode_reward)\n",
    "\n",
    "        return np.mean(total_reward)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "    # env = gym.make(\n",
    "    #     \"LunarLander-v2\",\n",
    "    #     continuous = False,\n",
    "    #     gravity = -10.0,\n",
    "    #     enable_wind = False,\n",
    "    #     wind_power = 15.0,\n",
    "    #     turbulence_power = 1.5\n",
    "    # )\n",
    "\n",
    "    # env = RedBlueDoorsEnv(size=8)\n",
    "    # agent = Agent(env)\n",
    "    # agent.train(20,env)\n",
    "    # print(\"we did it\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b3946a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RedBlueDoorsEnv(size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d94bc292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multigrid.envs.empty import EmptyEnv\n",
    "\n",
    "env = EmptyEnv(size=8)\n",
    "\n",
    "# agent = Agent(env)\n",
    "# agent.train(20,env)\n",
    "# print(\"we did it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2de33a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6616f5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'image': array([[[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]]]),\n",
       "  'direction': 0,\n",
       "  'mission': Mission(\"get to the green goal square\")}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93f1c841",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(state, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32, device\u001b[39m=\u001b[39;49mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not dict"
     ]
    }
   ],
   "source": [
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66e38b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0][\"image\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multigrid_stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
