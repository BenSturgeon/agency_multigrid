{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71818425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.algorithms import Algorithm\n",
    "from ray.rllib.utils.typing import AgentID\n",
    "from train import algorithm_config, get_checkpoint_dir, get_policy_mapping_fn\n",
    "from typing import Any, Callable, Iterable\n",
    "\n",
    "\n",
    "\n",
    "def get_actions(\n",
    "    agent_ids: Iterable[AgentID],\n",
    "    algorithm: Algorithm,\n",
    "    policy_mapping_fn: Callable[[AgentID], str],\n",
    "    observations: dict[AgentID, Any],\n",
    "    states: dict[AgentID, Any]) -> tuple[dict[AgentID, Any], dict[AgentID, Any]]:\n",
    "    \"\"\"\n",
    "    Get actions for the given agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent_ids : Iterable[AgentID]\n",
    "        Agent IDs for which to get actions\n",
    "    algorithm : Algorithm\n",
    "        RLlib algorithm instance with trained policies\n",
    "    policy_mapping_fn : Callable(AgentID) -> str\n",
    "        Function mapping agent IDs to policy IDs\n",
    "    observations : dict[AgentID, Any]\n",
    "        Observations for each agent\n",
    "    states : dict[AgentID, Any]\n",
    "        States for each agent\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    actions : dict[AgentID, Any]\n",
    "        Actions for each agent\n",
    "    states : dict[AgentID, Any]\n",
    "        Updated states for each agent\n",
    "    \"\"\"\n",
    "    actions = {}\n",
    "    for agent_id in agent_ids:\n",
    "        if states[agent_id]:\n",
    "            actions[agent_id], states[agent_id], _ = algorithm.compute_single_action(\n",
    "                observations[agent_id],\n",
    "                states[agent_id],\n",
    "                policy_id=policy_mapping_fn(agent_id)\n",
    "            )\n",
    "        else:\n",
    "            actions[agent_id] = algorithm.compute_single_action(\n",
    "                observations[agent_id],\n",
    "                policy_id=policy_mapping_fn(agent_id)\n",
    "            )\n",
    "\n",
    "    return actions, states\n",
    "\n",
    "def visualize(\n",
    "    algorithm: Algorithm,\n",
    "    policy_mapping_fn: Callable[[AgentID], str],\n",
    "    num_episodes: int = 10) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Visualize trajectories from trained agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    algorithm : Algorithm\n",
    "        RLlib algorithm instance with trained policies\n",
    "    policy_mapping_fn : Callable(AgentID) -> str\n",
    "        Function mapping agent IDs to policy IDs\n",
    "    num_episodes : int, default=10\n",
    "        Number of episodes to visualize\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    env = algorithm.env_creator(algorithm.config.env_config)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print('\\n', '-' * 32, '\\n', 'Episode', episode, '\\n', '-' * 32)\n",
    "\n",
    "        episode_rewards = {agent_id: 0.0 for agent_id in env.get_agent_ids()}\n",
    "        terminations, truncations = {'__all__': False}, {'__all__': False}\n",
    "        observations, infos = env.reset()\n",
    "        states = {\n",
    "            agent_id: algorithm.get_policy(policy_mapping_fn(agent_id)).get_initial_state()\n",
    "            for agent_id in env.get_agent_ids()\n",
    "        }\n",
    "        while not terminations['__all__'] and not truncations['__all__']:\n",
    "            frames.append(env.get_frame())\n",
    "            actions, states = get_actions(\n",
    "                env.get_agent_ids(), algorithm, policy_mapping_fn, observations, states)\n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "            for agent_id in rewards:\n",
    "                episode_rewards[agent_id] += rewards[agent_id]\n",
    "\n",
    "        frames.append(env.get_frame())\n",
    "        print('Rewards:', episode_rewards)\n",
    "\n",
    "    env.close()\n",
    "    return frames\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dad94c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 16:40:07,063\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-09-20 16:40:07,070\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 16:40:07,189\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-09-20 16:40:07,431\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-09-20 16:40:07,468\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-09-20 16:40:07,471\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define your parameters here instead of using argparse\n",
    "algo = 'PPO'\n",
    "framework = 'torch'\n",
    "lstm = False  # Use True if you want to enable LSTM\n",
    "env = 'MultiGrid-Empty-8x8-v0'\n",
    "env_config = {}  # You can define a custom config dictionary here\n",
    "num_agents = 2\n",
    "num_episodes = 10\n",
    "max_steps = 20\n",
    "load_dir = None  # Provide the directory if you have a pre-trained model\n",
    "gif = None  # Provide the path if you want to save the output as a GIF\n",
    "\n",
    "# # Rest of your functions (get_actions, visualize) remain the same\n",
    "\n",
    "# Equivalent of your main block\n",
    "args_env_config = {'render_mode': 'human'}\n",
    "config = algorithm_config(\n",
    "    algo=algo,\n",
    "    framework=framework,\n",
    "    lstm=lstm,\n",
    "    env=env,\n",
    "    env_config={**args_env_config, **env_config},\n",
    "    num_agents=num_agents,\n",
    "    num_episodes=num_episodes,\n",
    "    max_steps=max_steps,\n",
    "    load_dir=load_dir,\n",
    "    gif=gif,\n",
    "    num_workers=0,\n",
    "    num_gpus=0,\n",
    ")\n",
    "\n",
    "algorithm = config.build()\n",
    "checkpoint = get_checkpoint_dir(load_dir)\n",
    "policy_mapping_fn = lambda agent_id, *args, **kwargs: f'policy_{agent_id}'\n",
    "if checkpoint:\n",
    "    print(f\"Loading checkpoint from {checkpoint}\")\n",
    "    algorithm.restore(checkpoint)\n",
    "    policy_mapping_fn = get_policy_mapping_fn(checkpoint, num_agents)\n",
    "\n",
    "frames = visualize(algorithm, policy_mapping_fn, num_episodes=num_episodes)\n",
    "if gif:\n",
    "    from array2gif import write_gif\n",
    "    filename = gif if gif.endswith('.gif') else f'{gif}.gif'\n",
    "    print(f\"Saving GIF to {filename}\")\n",
    "    write_gif(np.array(frames), filename, fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336521fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 16:40:09,616\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-09-20 16:40:09,623\tINFO tensorboardx.py:48 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-09-20 16:40:09,624\tWARNING unified.py:56 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agents` for environment variables or `env.get_wrapper_attr('agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/bensturgeon/mambaforge/envs/multigrid_test/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "2023-09-20 16:40:12,271\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-09-20 16:40:12,281\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "2023-09-20 16:40:12,285\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!\n",
      "2023-09-20 16:40:12,304\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "2023-09-20 16:40:12,309\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.visionnet.VisionNetwork` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f495a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multigrid_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
